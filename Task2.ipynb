{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-12-17T17:42:20.806485Z","iopub.execute_input":"2023-12-17T17:42:20.806764Z","iopub.status.idle":"2023-12-17T17:42:24.160851Z","shell.execute_reply.started":"2023-12-17T17:42:20.806741Z","shell.execute_reply":"2023-12-17T17:42:24.159774Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    # q: 8 x 1024 x 288, k: 8 x 1024 x 288, v: 8 x 1024 x 288, mask 1024 x 1024\n    d_k = q.size()[-1] \n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 8 x 1024 x 1024\n    if mask is not None:\n        scaled += mask # 8 x 1024 x 1024\n    attention = F.softmax(scaled, dim=-1) # 8 x 1024 x 1024\n    values = torch.matmul(attention, v)\n    return values, attention","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:42:24.162929Z","iopub.execute_input":"2023-12-17T17:42:24.164034Z","iopub.status.idle":"2023-12-17T17:42:24.170033Z","shell.execute_reply.started":"2023-12-17T17:42:24.163996Z","shell.execute_reply":"2023-12-17T17:42:24.169008Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class GPT2Attention(nn.Module):\n    def __init__(self,d_model,num_heads,drop_prob):\n        super(GPT2Attention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.c_attn = nn.Linear(d_model , 3 * d_model)\n        self.c_proj = nn.Linear(d_model, d_model)\n        self.attn_dropout = nn.Dropout(p=drop_prob)\n    def forward(self, x):\n        # Implement the forward pass for GPT2Attention\n        sequence_length=1024\n        qkv = self.c_attn(x)\n        qkv = qkv.reshape(sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(1,0,2)\n        q, k, v = qkv.chunk(3, dim=-1)\n        mask = torch.full([1024,1024] , float('-inf'))\n        mask = torch.triu(mask, diagonal=1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.reshape(sequence_length, self.num_heads * self.head_dim)\n        out = self.c_proj(values)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:42:24.171398Z","iopub.execute_input":"2023-12-17T17:42:24.171657Z","iopub.status.idle":"2023-12-17T17:42:24.183813Z","shell.execute_reply.started":"2023-12-17T17:42:24.171636Z","shell.execute_reply":"2023-12-17T17:42:24.182977Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    def __init__(self,d_model,hidden,drop_prob):\n        super(GPT2MLP, self).__init__()\n        self.c_fc = nn.Linear(d_model, hidden)\n        self.c_proj = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x=self.c_fc(x)\n        x=self.c_proj(x)\n        x=self.relu(x)\n        x=self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:42:24.186253Z","iopub.execute_input":"2023-12-17T17:42:24.186716Z","iopub.status.idle":"2023-12-17T17:42:24.194653Z","shell.execute_reply.started":"2023-12-17T17:42:24.186685Z","shell.execute_reply":"2023-12-17T17:42:24.193880Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class GPT2Block(nn.Module):\n    def __init__(self,d_model,num_heads,fnn_hidden,drop_prob):\n        super(GPT2Block, self).__init__()\n\n        self.ln_1 = nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        # GPT2SelfAttention\n        self.attn = GPT2Attention(d_model,num_heads,drop_prob)\n        self.ln_2 = nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        # GPT2 MultiLayerPerceptron\n        self.mlp = GPT2MLP(d_model,fnn_hidden,drop_prob)\n\n    def forward(self, x):\n        x = self.ln_1(x)\n        x = self.attn(x)\n        x = self.ln_2(x)\n        x = self.mlp(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:42:24.195906Z","iopub.execute_input":"2023-12-17T17:42:24.196168Z","iopub.status.idle":"2023-12-17T17:42:24.204575Z","shell.execute_reply.started":"2023-12-17T17:42:24.196147Z","shell.execute_reply":"2023-12-17T17:42:24.203572Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class RotaryEmbedding(nn.Module):\n    def __init__(self, d_model):\n        super(RotaryEmbedding, self).__init__()\n        self.d_model = d_model\n        self.cos_embedding = nn.Embedding(d_model // 2, 1)\n        self.sin_embedding = nn.Embedding(d_model // 2, 1)\n\n    def forward(self, positions):\n        angles = positions / torch.pow(10000, torch.arange(0, self.d_model, 2).float() / self.d_model)\n        angles[:, 0::2] = torch.sin(angles[:, 0::2])\n        angles[:, 1::2] = torch.cos(angles[:, 1::2])\n        return angles.unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:42:24.205617Z","iopub.execute_input":"2023-12-17T17:42:24.205897Z","iopub.status.idle":"2023-12-17T17:42:24.217883Z","shell.execute_reply.started":"2023-12-17T17:42:24.205862Z","shell.execute_reply":"2023-12-17T17:42:24.217124Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class GPT2WithRotaryEmbedding(nn.Module):\n    def __init__(self, vocab_size=50257, embedding_dim=768,sequence_length=1024, num_blocks=12):\n        super(GPT2WithRotaryEmbedding, self).__init__()\n\n        # Embedding layer\n        self.embedding = nn.Embedding(sequence_length,768)\n        self.rotary_pos_emb = RotaryEmbedding(768)\n        self.drop = nn.Dropout(p=0.1)\n\n        # GPT2 Transformer blocks\n        self.h = nn.ModuleList([GPT2Block(d_model=embedding_dim,num_heads=8,fnn_hidden=768*4,drop_prob=0.1) for _ in range(num_blocks)])\n\n        # Final layer normalization\n        self.ln_f = nn.LayerNorm((embedding_dim,), eps=1e-05, elementwise_affine=True)\n\n    def forward(self, x):\n        # Input x should be a sequence of token indices\n        positions = torch.arange(x.size(1)).unsqueeze(0)\n        x = self.embedding(x) + self.rotary_pos_emb(positions)\n        \n        \n        # GPT2 Transformer blocks\n        for block in self.h:\n            embedded = block(embedded)\n\n        # Final layer normalization\n        output = self.ln_f(embedded)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:43:36.594263Z","iopub.execute_input":"2023-12-17T17:43:36.594931Z","iopub.status.idle":"2023-12-17T17:43:36.603171Z","shell.execute_reply.started":"2023-12-17T17:43:36.594901Z","shell.execute_reply":"2023-12-17T17:43:36.602264Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model=GPT2WithRotaryEmbedding()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:43:38.067926Z","iopub.execute_input":"2023-12-17T17:43:38.068292Z","iopub.status.idle":"2023-12-17T17:43:38.820400Z","shell.execute_reply.started":"2023-12-17T17:43:38.068263Z","shell.execute_reply":"2023-12-17T17:43:38.819615Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:43:43.279198Z","iopub.execute_input":"2023-12-17T17:43:43.279880Z","iopub.status.idle":"2023-12-17T17:43:43.288795Z","shell.execute_reply.started":"2023-12-17T17:43:43.279846Z","shell.execute_reply":"2023-12-17T17:43:43.287875Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"GPT2WithRotaryEmbedding(\n  (embedding): Embedding(1024, 768)\n  (rotary_pos_emb): RotaryEmbedding(\n    (cos_embedding): Embedding(384, 1)\n    (sin_embedding): Embedding(384, 1)\n  )\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0-11): 12 x GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (relu): ReLU()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Function to count the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:43:55.411868Z","iopub.execute_input":"2023-12-17T17:43:55.412237Z","iopub.status.idle":"2023-12-17T17:43:55.416995Z","shell.execute_reply.started":"2023-12-17T17:43:55.412209Z","shell.execute_reply":"2023-12-17T17:43:55.416054Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"count_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T17:43:56.691279Z","iopub.execute_input":"2023-12-17T17:43:56.692101Z","iopub.status.idle":"2023-12-17T17:43:56.698409Z","shell.execute_reply.started":"2023-12-17T17:43:56.692071Z","shell.execute_reply":"2023-12-17T17:43:56.697532Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"85843200"},"metadata":{}}]}]}